import os
import pandas as pd
import ast
import time
from dotenv import load_dotenv
from ragas import EvaluationDataset, evaluate
from ragas.llms import LangchainLLMWrapper
from ragas.metrics import LLMContextPrecisionWithReference, LLMContextPrecisionWithoutReference
from langchain_openai import ChatOpenAI
from langfuse import Langfuse
from langfuse.callback import CallbackHandler

# Ortam deÄŸiÅŸkenlerini yÃ¼kle
load_dotenv()
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")

# Langfuse baÅŸlatma
langfuse = Langfuse(
    public_key=os.getenv("LANGFUSE_PUBLIC_KEY"),
    secret_key=os.getenv("LANGFUSE_SECRET_KEY"),
    host=os.getenv("LANGFUSE_HOST", "https://cloud.langfuse.com")
)

# Langfuse callback handler
langfuse_handler = CallbackHandler(
    public_key=os.getenv("LANGFUSE_PUBLIC_KEY"),
    secret_key=os.getenv("LANGFUSE_SECRET_KEY"),
    host=os.getenv("LANGFUSE_HOST", "https://cloud.langfuse.com")
)

# LLM tanÄ±mÄ± - gpt-3.5-turbo kullanÄ±yoruz
llm = LangchainLLMWrapper(ChatOpenAI(
    model="gpt-3.5-turbo",
    callbacks=[langfuse_handler]
))

# CSV yolu
csv_path = "data/raw/ragas_ready_dataset_with_responses_UPDATED.csv"
df = pd.read_csv(csv_path)

# DeÄŸerlendirilecek 3 response tÃ¼rÃ¼
response_columns = {
    "response_normal": "context_precision_normal.csv",
    "response_no_exec": "context_precision_noexec.csv",
    "response_no_severity": "context_precision_noseverity.csv"
}

# Ortak metrikler
metrics = [
    LLMContextPrecisionWithReference(llm=llm),
    LLMContextPrecisionWithoutReference(llm=llm)
]

# Batch size ve bekleme sÃ¼resi
BATCH_SIZE = 5
WAIT_TIME = 2  # saniye

# Her model cevabÄ± iÃ§in dÃ¶ngÃ¼
for response_col, output_file in response_columns.items():
    samples = []
    for idx, row in df.iterrows():
        try:
            # BoÅŸ response deÄŸerlerini kontrol et
            if pd.isna(row[response_col]):
                print(f"âš ï¸ SatÄ±r {idx}: BoÅŸ response deÄŸeri atlandÄ±.")
                continue
                
            contexts = ast.literal_eval(row["retrieved_contexts"])
            samples.append({
                "user_input": row["user_input"],
                "retrieved_contexts": contexts,
                "response": row[response_col],
                "reference": row["reference"]
            })
            
            # Batch size kontrolÃ¼
            if len(samples) >= BATCH_SIZE:
                print(f"ğŸ” {response_col} iÃ§in {len(samples)} Ã¶rnek deÄŸerlendiriliyor...")
                dataset = EvaluationDataset.from_list(samples)
                results = evaluate(dataset, metrics=metrics, llm=llm)
                results_df = results.to_pandas()
                results_df.to_csv(output_file, mode='a', header=not os.path.exists(output_file), index=False)
                print(f"âœ… Batch deÄŸerlendirmesi tamamlandÄ± â†’ {output_file}")
                samples = []  # Batch'i temizle
                time.sleep(WAIT_TIME)  # Rate limit iÃ§in bekle
                
        except Exception as e:
            print(f"âš ï¸ Hata satÄ±r {idx}: {e}")
            continue

    # Kalan Ã¶rnekleri deÄŸerlendir
    if samples:
        print(f"ğŸ” {response_col} iÃ§in kalan {len(samples)} Ã¶rnek deÄŸerlendiriliyor...")
        dataset = EvaluationDataset.from_list(samples)
        results = evaluate(dataset, metrics=metrics, llm=llm)
        results_df = results.to_pandas()
        results_df.to_csv(output_file, mode='a', header=not os.path.exists(output_file), index=False)
        print(f"âœ… Son batch deÄŸerlendirmesi tamamlandÄ± â†’ {output_file}")

# Langfuse'u kapat
langfuse.flush()
